{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNwo7HsgIr/B53yNzh0k+Kg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mikhelmariya/ML/blob/main/Named_Entity_Recognition_Natural_Language_Processing_with_python_and_NLTK.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WtG9ENnYcGAj",
        "outputId": "37329801-a260-4ba0-9bc6-fea832b98dd4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package state_union to /root/nltk_data...\n",
            "[nltk_data]   Package state_union is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "#following initial steps as explained in previous chapters\n",
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger') #part of speech tagging\n",
        "nltk.download('state_union') #datasets\n",
        "nltk.download('punkt')  #for word,sentence tokenizer\n",
        "nltk.download('maxent_ne_chunker') #for using ne_chunk() function on tagged words for grouping into named entity groups\n",
        "nltk.download('words') \n",
        "from nltk.corpus import state_union\n",
        "from nltk.tokenize import PunktSentenceTokenizer\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data=state_union.raw(\"2005-GWBush.txt\") #training dataset\n",
        "test_data = state_union.raw(\"2006-GWBush.txt\") #testing dataset\n",
        "trained = PunktSentenceTokenizer(train_data) \n",
        "tokenized=trained.tokenize(test_data)"
      ],
      "metadata": {
        "id": "SAhwZ0TJfFBi"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Here the function name is NER() and we are following try-catch mechanism to deal with exceptions\n",
        "def  NER():\n",
        "  try:\n",
        "    for i in tokenized[:3]:  #looping over  text\n",
        "       words=nltk.word_tokenize(i)  #tokenizing into words\n",
        "       tagged=nltk.pos_tag(words)  #tagging each words\n",
        "       namedentity=nltk.ne_chunk(tagged) #grouping based on named entity\n",
        "       print(namedentity) \n",
        "  except Exception as e :\n",
        "     print(str(e))  \n",
        "\n",
        "NER() #function call"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Oq5J-oxgsxu",
        "outputId": "c549e21c-aa91-4dda-883a-d7417213def1"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(S\n",
            "  PRESIDENT/NNP\n",
            "  (PERSON GEORGE/NNP W./NNP BUSH/NNP)\n",
            "  'S/POS\n",
            "  (ORGANIZATION ADDRESS/NNP)\n",
            "  BEFORE/IN\n",
            "  A/NNP\n",
            "  (ORGANIZATION JOINT/NNP)\n",
            "  SESSION/NNP\n",
            "  OF/IN\n",
            "  (ORGANIZATION THE/NNP)\n",
            "  (ORGANIZATION CONGRESS/NNP)\n",
            "  ON/NNP\n",
            "  THE/NNP\n",
            "  (ORGANIZATION STATE/NNP OF/IN)\n",
            "  (ORGANIZATION THE/NNP)\n",
            "  (ORGANIZATION UNION/NNP)\n",
            "  January/NNP\n",
            "  31/CD\n",
            "  ,/,\n",
            "  2006/CD\n",
            "  (ORGANIZATION THE/NNP)\n",
            "  PRESIDENT/NNP\n",
            "  :/:\n",
            "  Thank/NNP\n",
            "  you/PRP\n",
            "  all/DT\n",
            "  ./.)\n",
            "(S\n",
            "  (PERSON Mr./NNP Speaker/NNP)\n",
            "  ,/,\n",
            "  Vice/NNP\n",
            "  President/NNP\n",
            "  (PERSON Cheney/NNP)\n",
            "  ,/,\n",
            "  members/NNS\n",
            "  of/IN\n",
            "  (ORGANIZATION Congress/NNP)\n",
            "  ,/,\n",
            "  members/NNS\n",
            "  of/IN\n",
            "  the/DT\n",
            "  (ORGANIZATION Supreme/NNP Court/NNP)\n",
            "  and/CC\n",
            "  diplomatic/JJ\n",
            "  corps/NN\n",
            "  ,/,\n",
            "  distinguished/JJ\n",
            "  guests/NNS\n",
            "  ,/,\n",
            "  and/CC\n",
            "  fellow/JJ\n",
            "  citizens/NNS\n",
            "  :/:\n",
            "  Today/VB\n",
            "  our/PRP$\n",
            "  nation/NN\n",
            "  lost/VBD\n",
            "  a/DT\n",
            "  beloved/VBN\n",
            "  ,/,\n",
            "  graceful/JJ\n",
            "  ,/,\n",
            "  courageous/JJ\n",
            "  woman/NN\n",
            "  who/WP\n",
            "  called/VBD\n",
            "  (GPE America/NNP)\n",
            "  to/TO\n",
            "  its/PRP$\n",
            "  founding/NN\n",
            "  ideals/NNS\n",
            "  and/CC\n",
            "  carried/VBD\n",
            "  on/IN\n",
            "  a/DT\n",
            "  noble/JJ\n",
            "  dream/NN\n",
            "  ./.)\n",
            "(S\n",
            "  Tonight/NN\n",
            "  we/PRP\n",
            "  are/VBP\n",
            "  comforted/VBN\n",
            "  by/IN\n",
            "  the/DT\n",
            "  hope/NN\n",
            "  of/IN\n",
            "  a/DT\n",
            "  glad/JJ\n",
            "  reunion/NN\n",
            "  with/IN\n",
            "  the/DT\n",
            "  husband/NN\n",
            "  who/WP\n",
            "  was/VBD\n",
            "  taken/VBN\n",
            "  so/RB\n",
            "  long/RB\n",
            "  ago/RB\n",
            "  ,/,\n",
            "  and/CC\n",
            "  we/PRP\n",
            "  are/VBP\n",
            "  grateful/JJ\n",
            "  for/IN\n",
            "  the/DT\n",
            "  good/JJ\n",
            "  life/NN\n",
            "  of/IN\n",
            "  (ORGANIZATION Coretta/NNP Scott/NNP King/NNP)\n",
            "  ./.)\n"
          ]
        }
      ]
    }
  ]
}