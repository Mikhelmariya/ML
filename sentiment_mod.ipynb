{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mikhelmariya/ML/blob/main/sentiment_mod.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F6Jwmd00KIHG",
        "outputId": "710d2eb0-0336-40cb-8e5e-8f45e09339c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "import random\n",
        "from nltk.corpus import movie_reviews\n",
        "from nltk.classify.scikitlearn import SklearnClassifier\n",
        "import pickle\n",
        "\n",
        "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
        "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
        "from sklearn.svm import SVC, LinearSVC, NuSVC\n",
        "\n",
        "from nltk.classify import ClassifierI\n",
        "from statistics import mode\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import io"
      ],
      "metadata": {
        "id": "Z4WqACQZOeMy"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class VoteClassifier(ClassifierI):\n",
        "    def __init__(self, *classifiers):\n",
        "        self._classifiers = classifiers\n",
        "\n",
        "    def classify(self, features):\n",
        "        votes = []\n",
        "        for c in self._classifiers:\n",
        "            v = c.classify(features)\n",
        "            votes.append(v)\n",
        "        return mode(votes)\n",
        "\n",
        "    def confidence(self, features):\n",
        "        votes = []\n",
        "        for c in self._classifiers:\n",
        "            v = c.classify(features)\n",
        "            votes.append(v)\n",
        "\n",
        "        choice_votes = votes.count(mode(votes))\n",
        "        conf = choice_votes / len(votes)\n",
        "        return conf"
      ],
      "metadata": {
        "id": "cxhgRfyfKVBT"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!mkdir short_reviews #creating folder  short_reviews"
      ],
      "metadata": {
        "id": "58kRk_euLvEL"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with io.open(\"/content/short_reviews/positive.txt\", encoding='latin-1') as f:\n",
        "  short_pos = f.read() # open the file positive and read"
      ],
      "metadata": {
        "id": "KXOvo6ClOYSm"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with io.open(\"/content/short_reviews/negative.txt\", encoding='latin-1') as f:\n",
        "  short_neg = f.read()#open the file negative and read"
      ],
      "metadata": {
        "id": "W3M1i1eWOuM_"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "documents = [] #empty list\n",
        "#appending lines of text from both the files\n",
        "for r in short_pos.split('\\n'):\n",
        "    documents.append( (r, \"pos\") )\n",
        "\n",
        "for r in short_neg.split('\\n'):\n",
        "    documents.append( (r, \"neg\") )"
      ],
      "metadata": {
        "id": "WucWZRy9O2rz"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_words = []\n",
        "#tokenizing  the text\n",
        "\n",
        "short_pos_words = word_tokenize(short_pos)\n",
        "short_neg_words = word_tokenize(short_neg)"
      ],
      "metadata": {
        "id": "1hqaBVi2O8h0"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#part of speech tagging\n",
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vsUHGpIcs2BB",
        "outputId": "2732b9ce-9187-4e00-d477-226d17eaeb0c"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#  j is adject, r is adverb, and v is verb\n",
        "#allowed_word_types = [\"J\",\"R\",\"V\"]\n",
        "allowed_word_types = [\"J\"]\n",
        "for p in short_pos.split('\\n'):\n",
        "    documents.append( (p, \"pos\") )\n",
        "    words = word_tokenize(p)\n",
        "    pos = nltk.pos_tag(words)\n",
        "    for w in pos:\n",
        "        if w[1][0] in allowed_word_types:\n",
        "            all_words.append(w[0].lower())"
      ],
      "metadata": {
        "id": "dud_IUGyraaV"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for p in short_neg.split('\\n'):\n",
        "    documents.append( (p, \"neg\") )\n",
        "    words = word_tokenize(p)\n",
        "    pos = nltk.pos_tag(words)\n",
        "    for w in pos:\n",
        "        if w[1][0] in allowed_word_types:\n",
        "            all_words.append(w[0].lower())"
      ],
      "metadata": {
        "id": "G0tO0dxJroMN"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir pickled_algos # directory for saving pickled files"
      ],
      "metadata": {
        "id": "rC3z9D9GvUas"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "save_documents = open(\"pickled_algos/documents.pickle\",\"wb\")\n",
        "pickle.dump(documents, save_documents)\n",
        "save_documents.close()"
      ],
      "metadata": {
        "id": "oKp9-kfqvODU"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for w in short_pos_words:\n",
        "    all_words.append(w.lower())\n",
        "\n",
        "for w in short_neg_words:\n",
        "    all_words.append(w.lower())"
      ],
      "metadata": {
        "id": "OUnDFfk0PKOZ"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_words = nltk.FreqDist(all_words)"
      ],
      "metadata": {
        "id": "kkozNsNrPNDz"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_features = list(all_words.keys())[:5000]\n",
        "\n",
        "def find_features(document):\n",
        "    words = word_tokenize(document)\n",
        "    features = {}\n",
        "    for w in word_features:\n",
        "        features[w] = (w in words)\n",
        "\n",
        "    return features\n",
        "\t\n",
        "featuresets = [(find_features(rev), category) for (rev, category) in documents]\n",
        "random.shuffle(featuresets)"
      ],
      "metadata": {
        "id": "YHcJNfhuPQsD"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#saving word_features\n",
        "save_word_features = open(\"pickled_algos/word_features5k.pickle\",\"wb\")\n",
        "pickle.dump(word_features, save_word_features)\n",
        "save_word_features.close()"
      ],
      "metadata": {
        "id": "p33Dqd6evy98"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_set = featuresets[:10000]\n",
        "testing_set =  featuresets[10000:]"
      ],
      "metadata": {
        "id": "Q5jQY-hJPeGb"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classifier = nltk.NaiveBayesClassifier.train(training_set)\n",
        "print(\"Original Naive Bayes Algo accuracy percent:\", (nltk.classify.accuracy(classifier, testing_set))*100)\n",
        "classifier.show_most_informative_features(15)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5M3FHsxvPhiU",
        "outputId": "0946d288-d58c-47f2-975b-b2e1393c210a"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Naive Bayes Algo accuracy percent: 77.10981638418079\n",
            "Most Informative Features\n",
            "              engrossing = True              pos : neg    =     19.8 : 1.0\n",
            "                    loud = True              neg : pos    =     18.2 : 1.0\n",
            "                    dull = True              neg : pos    =     18.0 : 1.0\n",
            "                    flat = True              neg : pos    =     16.9 : 1.0\n",
            "                mindless = True              neg : pos    =     14.2 : 1.0\n",
            "              refreshing = True              pos : neg    =     13.8 : 1.0\n",
            "                 generic = True              neg : pos    =     13.6 : 1.0\n",
            "                 amazing = True              pos : neg    =     13.1 : 1.0\n",
            "               inventive = True              pos : neg    =     13.1 : 1.0\n",
            "                    open = True              pos : neg    =     13.1 : 1.0\n",
            "                 playful = True              pos : neg    =     12.4 : 1.0\n",
            "               offensive = True              neg : pos    =     12.3 : 1.0\n",
            "                    poor = True              neg : pos    =     12.3 : 1.0\n",
            "                   quiet = True              pos : neg    =     11.9 : 1.0\n",
            "               realistic = True              pos : neg    =     11.7 : 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# saving naivebayes classifier\n",
        "save_classifier = open(\"pickled_algos/originalnaivebayes5k.pickle\",\"wb\")\n",
        "pickle.dump(classifier, save_classifier)\n",
        "save_classifier.close()"
      ],
      "metadata": {
        "id": "CgOKnTJfv8kf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MNB_classifier = SklearnClassifier(MultinomialNB())\n",
        "MNB_classifier.train(training_set)\n",
        "print(\"MNB_classifier accuracy percent:\", (nltk.classify.accuracy(MNB_classifier, testing_set))*100)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1PNIpWAGQspZ",
        "outputId": "eefd1caa-82cb-40a7-d5c7-0872109b415b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MNB_classifier accuracy percent: 76.49395357048283\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "save_classifier = open(\"pickled_algos/MNB_classifier5k.pickle\",\"wb\")\n",
        "pickle.dump(MNB_classifier, save_classifier)\n",
        "save_classifier.close()\n"
      ],
      "metadata": {
        "id": "0K_65ykewS2I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BernoulliNB_classifier = SklearnClassifier(BernoulliNB())\n",
        "BernoulliNB_classifier.train(training_set)\n",
        "print(\"BernoulliNB_classifier accuracy percent:\", (nltk.classify.accuracy(BernoulliNB_classifier, testing_set))*100)\n",
        "\n",
        "LogisticRegression_classifier = SklearnClassifier(LogisticRegression())\n",
        "LogisticRegression_classifier.train(training_set)\n",
        "print(\"LogisticRegression_classifier accuracy percent:\", (nltk.classify.accuracy(LogisticRegression_classifier, testing_set))*100)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rdRb62Z6Ra0l",
        "outputId": "03a05905-a92a-49de-f693-989e825d4d8b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BernoulliNB_classifier accuracy percent: 76.85585665107247\n",
            "LogisticRegression_classifier accuracy percent: 77.22658663606673\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "save_classifier = open(\"pickled_algos/BernoulliNB_classifier5k.pickle\",\"wb\")\n",
        "pickle.dump(BernoulliNB_classifier, save_classifier)\n",
        "save_classifier.close()\n"
      ],
      "metadata": {
        "id": "QHrsz8AbwWLY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "voted_classifier = VoteClassifier(\n",
        "                                  MNB_classifier,\n",
        "                                  BernoulliNB_classifier,\n",
        "                                  LogisticRegression_classifier)\n",
        "\n",
        "print(\"voted_classifier accuracy percent:\", (nltk.classify.accuracy(voted_classifier, testing_set))*100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "20ivrbr3SKFb",
        "outputId": "79ac0c4d-21dd-410f-fcf9-4ced77d7c1f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "voted_classifier accuracy percent: 76.81172212904934\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#loading pickled files\n",
        "documents_f = open(\"pickled_algos/documents.pickle\", \"rb\")\n",
        "documents = pickle.load(documents_f)\n",
        "documents_f.close()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "word_features5k_f = open(\"pickled_algos/word_features5k.pickle\", \"rb\")\n",
        "word_features = pickle.load(word_features5k_f)\n",
        "word_features5k_f.close()"
      ],
      "metadata": {
        "id": "xgC6DFYs7pMe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "open_file = open(\"pickled_algos/originalnaivebayes5k.pickle\", \"rb\")\n",
        "classifier = pickle.load(open_file)\n",
        "open_file.close()\n",
        "\n",
        "\n",
        "open_file = open(\"pickled_algos/MNB_classifier5k.pickle\", \"rb\")\n",
        "MNB_classifier = pickle.load(open_file)\n",
        "open_file.close()\n",
        "\n",
        "\n",
        "\n",
        "open_file = open(\"pickled_algos/BernoulliNB_classifier5k.pickle\", \"rb\")\n",
        "BernoulliNB_classifier = pickle.load(open_file)\n",
        "open_file.close()\n",
        "\n"
      ],
      "metadata": {
        "id": "mlBJTXnF7zzh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sentiment(text):\n",
        "    feats = find_features(text)\n",
        "    return voted_classifier.classify(feats),voted_classifier.confidence(feats)"
      ],
      "metadata": {
        "id": "jVZlYgVHy6aI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(sentiment(\"This movie was awesome! The acting was great, plot was wonderful, and there were pythons...so yea!\"))\n",
        "print(sentiment(\"This movie was utter junk. There were absolutely 0 pythons. I don't see what the point was at all. Horrible movie, 0/10\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0606vv9-FiVJ",
        "outputId": "92ea1ef9-17d9-4b69-dc9c-c9ffef67209c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('pos', 1.0)\n",
            "('neg', 1.0)\n"
          ]
        }
      ]
    }
  ]
}